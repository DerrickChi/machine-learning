{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import display, Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from scipy import stats\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (26721, 32, 32, 1), (26721, 5))\n",
      "('Validation set', (6680, 32, 32, 1), (6680, 5))\n",
      "('Test set', (13068, 32, 32, 1), (13068, 5))\n"
     ]
    }
   ],
   "source": [
    "with open('SVHN_data.pickle', 'rb') as f:\n",
    "    tmp = pickle.load(f)\n",
    "    train_dataset = tmp['train_dataset']\n",
    "    train_labels = tmp['train_labels']\n",
    "    valid_dataset = tmp['valid_dataset']\n",
    "    valid_labels = tmp['valid_labels']\n",
    "    test_dataset = tmp['test_dataset']\n",
    "    test_labels = tmp['test_labels']\n",
    "    del tmp\n",
    "    \n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "num_labels = 11  # 0-9, + blank \n",
    "num_channels = 1 # grayscale\n",
    "batch_size = 64\n",
    "patch_size = 5\n",
    "depthC1 = 16\n",
    "depthC2 = 32\n",
    "depthC3 = 64\n",
    "depthC4 = 96\n",
    "depthFC1 = 128\n",
    "kp=0.9\n",
    "#beta = 0.1\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size, 5))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_valid_labels = tf.constant(valid_labels)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_test_labels = tf.constant(test_labels)\n",
    "\n",
    "    # Variables.\n",
    "    W_conv1 = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depthC1],stddev=0.1))\n",
    "    b_conv1 = tf.Variable(tf.ones([depthC1]))\n",
    "    \n",
    "    W_conv2 = tf.Variable(tf.truncated_normal([patch_size, patch_size, depthC1, depthC2],stddev=0.1))\n",
    "    b_conv2 = tf.Variable(tf.ones([depthC2]))\n",
    "    \n",
    "    W_conv3 = tf.Variable(tf.truncated_normal([patch_size, patch_size,depthC2, depthC3],stddev=0.1))\n",
    "    b_conv3 = tf.Variable(tf.ones([depthC3]))\n",
    "    \n",
    "    W_conv4 = tf.Variable(tf.truncated_normal([patch_size, patch_size,depthC3, depthC4],stddev=0.1))\n",
    "    b_conv4 = tf.Variable(tf.ones([depthC4]))\n",
    "\n",
    "    W_fc1 = tf.Variable(tf.truncated_normal([2*2*depthC4, depthFC1],stddev=0.1))\n",
    "    b_fc1 = tf.Variable(tf.ones([depthFC1]))\n",
    "    \n",
    "    W_o1 = tf.Variable(tf.truncated_normal([depthFC1, num_labels],stddev=0.1))\n",
    "    b_o1 = tf.Variable(tf.ones([num_labels]))\n",
    "    \n",
    "    W_o2 = tf.Variable(tf.truncated_normal([depthFC1, num_labels],stddev=0.1))\n",
    "    b_o2 = tf.Variable(tf.ones([num_labels]))\n",
    "    \n",
    "    W_o3 = tf.Variable(tf.truncated_normal([depthFC1, num_labels],stddev=0.1))\n",
    "    b_o3 = tf.Variable(tf.ones([num_labels]))\n",
    "    \n",
    "    W_o4 = tf.Variable(tf.truncated_normal([depthFC1, num_labels],stddev=0.1))\n",
    "    b_o4 = tf.Variable(tf.ones([num_labels]))\n",
    "    \n",
    "    W_o5 = tf.Variable(tf.truncated_normal([depthFC1, num_labels],stddev=0.1))\n",
    "    b_o5 = tf.Variable(tf.ones([num_labels]))\n",
    "  \n",
    "    # CNN\n",
    "    def model(data, keep_prob=1):    \n",
    "        #CONV\n",
    "        h_conv1 = tf.nn.conv2d(data,W_conv1, [1,1,1,1],padding='SAME', name='conv_layer1') + b_conv1\n",
    "        h_conv1 = tf.nn.relu(h_conv1)\n",
    "        h_conv1 = tf.nn.lrn(h_conv1) \n",
    "        h_conv1 = tf.nn.max_pool(h_conv1, [1,2,2,1], [1,2,2,1], 'SAME') \n",
    "        \n",
    "        h_conv2 = tf.nn.conv2d(h_conv1, W_conv2, [1,1,1,1], padding='SAME', name='conv_layer2') + b_conv2 \n",
    "        h_conv2 = tf.nn.relu(h_conv2)\n",
    "        h_conv2 = tf.nn.lrn(h_conv2) \n",
    "        h_conv2 = tf.nn.max_pool(h_conv2, [1,2,2,1], [1,2,2,1], 'SAME')\n",
    "        \n",
    "        h_conv3 = tf.nn.conv2d(h_conv2, W_conv3, [1,1,1,1], padding='SAME', name='conv_layer3') + b_conv3\n",
    "        h_conv3 = tf.nn.relu(h_conv3)\n",
    "        h_conv3 = tf.nn.lrn(h_conv3)\n",
    "        h_conv3 = tf.nn.max_pool(h_conv3, [1,2,2,1], [1,2,2,1], 'SAME')\n",
    "        \n",
    "        h_conv4 = tf.nn.conv2d(h_conv3, W_conv4, [1,1,1,1], padding='SAME', name='conv_layer4') + b_conv4\n",
    "        h_conv4 = tf.nn.relu(h_conv4)\n",
    "        h_conv4 = tf.nn.lrn(h_conv4)\n",
    "        h_conv4 = tf.nn.max_pool(h_conv4, [1,2,2,1], [1,2,2,1], 'SAME')\n",
    "        \n",
    "        h_conv4 = tf.nn.dropout(h_conv4, keep_prob)\n",
    "        \n",
    "        #Reshape\n",
    "        shape = h_conv4.get_shape().as_list()\n",
    "        h_conv4 = tf.reshape(h_conv4, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        \n",
    "        #FC\n",
    "        h_fc1 = tf.matmul(h_conv4, W_fc1) + b_fc1\n",
    "        h_fc1 = tf.nn.relu(h_fc1)\n",
    "        h_fc1 = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "        #OUTPUT\n",
    "        logits1 = tf.matmul(h_fc1, W_o1) + b_o1\n",
    "        logits2 = tf.matmul(h_fc1, W_o2) + b_o2\n",
    "        logits3 = tf.matmul(h_fc1, W_o3) + b_o3\n",
    "        logits4 = tf.matmul(h_fc1, W_o4) + b_o4\n",
    "        logits5 = tf.matmul(h_fc1, W_o5) + b_o5\n",
    "        return tf.pack([logits1, logits2, logits3, logits4, logits5])\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset, kp) \n",
    "    \n",
    "    # Define loss function.\n",
    "    with tf.name_scope(\"loss_function\") as scope:\n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[0], tf_train_labels[:,0])) +\\\n",
    "               tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[1], tf_train_labels[:,1])) +\\\n",
    "               tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[2], tf_train_labels[:,2])) +\\\n",
    "               tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[3], tf_train_labels[:,3])) +\\\n",
    "               tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[4], tf_train_labels[:,4]))\n",
    "        # Create a summary to monitor the cost function\n",
    "        tf.scalar_summary(\"loss_function\", loss)\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer().minimize(loss)\n",
    "    \n",
    "    def eval_accuracy(predictions, labels):\n",
    "        return tf.reduce_mean( tf.reduce_min(tf.to_float(tf.equal(tf.to_int32(predictions), labels)), axis = 1))\n",
    "\n",
    "\n",
    "    # Predictions for the mini training, validation, and test data.\n",
    "    train_prediction = tf.transpose(tf.argmax(logits, axis = 2))\n",
    "    valid_prediction =  tf.transpose(tf.argmax(model(tf_valid_dataset), axis = 2))\n",
    "    test_prediction =  tf.transpose(tf.argmax(model(tf_test_dataset), axis = 2))\n",
    "\n",
    "    train_accuracy = eval_accuracy(train_prediction, tf_train_labels)\n",
    "    valid_accuracy = eval_accuracy(valid_prediction, tf_valid_labels)\n",
    "    test_accuracy = eval_accuracy(test_prediction, tf_test_labels)\n",
    "    \n",
    "     # Create summaries to monitor the accuracy\n",
    "    tf.scalar_summary(\"mini-batch accuracy\", train_accuracy)\n",
    "    tf.scalar_summary(\"validation accuracy\", valid_accuracy)\n",
    "        \n",
    "    # Merge all summaries into a single operator\n",
    "    merged_summary_op = tf.merge_all_summaries()\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 12.580278\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.1%\n",
      "\n",
      "Minibatch loss at step 500: 5.146176\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 8.7%\n",
      "\n",
      "Minibatch loss at step 1000: 3.704948\n",
      "Minibatch accuracy: 32.8%\n",
      "Validation accuracy: 39.4%\n",
      "\n",
      "Minibatch loss at step 1500: 2.200338\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 52.2%\n",
      "\n",
      "Minibatch loss at step 2000: 1.941166\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 56.8%\n",
      "\n",
      "Minibatch loss at step 2500: 1.731720\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 59.9%\n",
      "\n",
      "Minibatch loss at step 3000: 1.737435\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 62.7%\n",
      "\n",
      "Minibatch loss at step 3500: 1.293933\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 61.1%\n",
      "\n",
      "Minibatch loss at step 4000: 1.127986\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 63.4%\n",
      "\n",
      "Minibatch loss at step 4500: 1.440659\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 63.1%\n",
      "\n",
      "Minibatch loss at step 5000: 0.942206\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 64.6%\n",
      "\n",
      "Minibatch loss at step 5500: 1.019441\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 64.7%\n",
      "\n",
      "Minibatch loss at step 6000: 0.741786\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 64.2%\n",
      "\n",
      "Minibatch loss at step 6500: 0.972135\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 65.8%\n",
      "\n",
      "Minibatch loss at step 7000: 0.869751\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 65.8%\n",
      "\n",
      "Minibatch loss at step 7500: 0.804891\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 64.9%\n",
      "\n",
      "Minibatch loss at step 8000: 0.546980\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 67.0%\n",
      "\n",
      "Minibatch loss at step 8500: 0.694271\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 65.9%\n",
      "\n",
      "Minibatch loss at step 9000: 0.406495\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 65.7%\n",
      "\n",
      "Minibatch loss at step 9500: 0.642455\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 64.2%\n",
      "\n",
      "Minibatch loss at step 10000: 0.349035\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 66.3%\n",
      "\n",
      "Minibatch loss at step 10500: 0.381652\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 66.9%\n",
      "\n",
      "Minibatch loss at step 11000: 0.471676\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 65.7%\n",
      "\n",
      "Minibatch loss at step 11500: 0.668210\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 66.4%\n",
      "\n",
      "Minibatch loss at step 12000: 0.583999\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 66.1%\n",
      "\n",
      "Minibatch loss at step 12500: 0.272706\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 66.0%\n",
      "\n",
      "Minibatch loss at step 13000: 0.565427\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 66.8%\n",
      "\n",
      "Minibatch loss at step 13500: 0.382314\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 66.7%\n",
      "\n",
      "Minibatch loss at step 14000: 0.490867\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 65.7%\n",
      "\n",
      "Minibatch loss at step 14500: 0.489104\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 66.7%\n",
      "\n",
      "Minibatch loss at step 15000: 0.428607\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 66.5%\n",
      "\n",
      "Minibatch loss at step 15500: 0.422508\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 66.7%\n",
      "\n",
      "Minibatch loss at step 16000: 0.556422\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 65.8%\n",
      "\n",
      "Minibatch loss at step 16500: 0.455144\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 66.2%\n",
      "\n",
      "Minibatch loss at step 17000: 0.204804\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 66.1%\n",
      "\n",
      "Minibatch loss at step 17500: 0.378306\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 66.3%\n",
      "\n",
      "Minibatch loss at step 18000: 0.284936\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 66.6%\n",
      "\n",
      "Minibatch loss at step 18500: 0.286392\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 66.4%\n",
      "\n",
      "Minibatch loss at step 19000: 0.190743\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 66.3%\n",
      "\n",
      "Minibatch loss at step 19500: 0.301517\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 65.9%\n",
      "\n",
      "Minibatch loss at step 20000: 0.296226\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 66.2%\n",
      "\n",
      "Test accuracy: 64.2%\n",
      "Model saved in file: ./ckpt_folder/CNN_trained_finalModel.ckpt-20000\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "#num_steps = 101\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    summary_writer = tf.train.SummaryWriter(\"./logs\", session.graph)\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size),:]\n",
    "        \n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        if (step % 500 ==0):\n",
    "            _, l, train_acc, summary_str = session.run([train_step, loss, train_accuracy, merged_summary_op], feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "        else:\n",
    "            _, l, train_acc = session.run([train_step, loss, train_accuracy], feed_dict=feed_dict)\n",
    "           \n",
    "        \n",
    "        #print('step: %d' % step),\n",
    "        if (step % 500 == 0):  #500\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % (train_acc*100))\n",
    "            print('Validation accuracy: %.1f%%' % (valid_accuracy.eval()*100))\n",
    "            print('')\n",
    "            \n",
    "    print('Test accuracy: %.1f%%' % (test_accuracy.eval()*100))\n",
    "    save_path = saver.save(session, \"./ckpt_folder/CNN_trained_finalModel.ckpt\",global_step=step)\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
